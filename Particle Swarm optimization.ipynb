{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3cd7b06-c28f-4efe-81ef-ebf7686cdc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "import requests\n",
    "from math import radians\n",
    "from tqdm.notebook import tqdm\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab8bb3f-399e-4c06-9079-38d519f853dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "<h1>Particle Swarm optimization application</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e242c82-ae7b-4b42-88b1-0935812e2e04",
   "metadata": {},
   "source": [
    "<h2>Reading dataset</h2>\n",
    "For the purpose of the experiment, the results are only shown for a particular cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486cd6b8-2cbf-4541-9152-b5e09385f8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Olist_data/clusters_new_features.csv', index_col= 0)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb629b7-f188-44fa-830c-56e8043476ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = df[df.CLUSTER_hdbscan == 86]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208d9437-6c1d-453a-abfe-5f6d01a70498",
   "metadata": {},
   "source": [
    "<h2>Plotting cluster on a map</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a2000da-7a1c-4ca1-b78b-6faf36a1ec7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapf = folium.Map(\n",
    "    location= [c1.geolocation_lat.mean(),c1.geolocation_lng.mean()],\n",
    "    zoom_start = 11.5,\n",
    "    tiles= 'OpenStreetMap',\n",
    "    height= 650\n",
    ")\n",
    "\n",
    "circles= c1.apply(\n",
    "    lambda row: folium.CircleMarker(\n",
    "        location= [row.geolocation_lat, row.geolocation_lng],\n",
    "        radius= 1,\n",
    "        popup= \"\"+str(row.geolocation_lat)+\", \"+str(row.geolocation_lng)+\"\\n\"+str(row.CLUSTER_hdbscan),\n",
    "        color= 'darkcyan',\n",
    "        fill= True,\n",
    "        fill_color= 'darkcyan',\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(mapf),\n",
    "    axis= 1\n",
    ")\n",
    "\n",
    "rect= folium.Rectangle(\n",
    "    bounds= [(c1.geolocation_lat.min(),c1.geolocation_lng.min()),(c1.geolocation_lat.max(), c1.geolocation_lng.max())]\n",
    ").add_to(mapf)\n",
    "\n",
    "cluster= folium.CircleMarker(\n",
    "    location= [c1.centroid_lat.values[0],c1.centroid_lng.values[0]],\n",
    "    radius= 5,\n",
    "    color= 'tomato',\n",
    "    fill= True,\n",
    "    fill_color= 'tomato',\n",
    "    fill_opacity=0.5\n",
    ").add_to(mapf)\n",
    "\n",
    "\n",
    "mapf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce065f8-8d63-482f-83c2-5abd39492968",
   "metadata": {},
   "source": [
    "<h2>Feature engineering functions</h2>\n",
    "Functions to calculate Haversine distance and retrieve road distance and road duration using Google Maps API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b726836f-2b65-40c9-8001-d1a59e218b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "maps_api_key = 'ENTER YOUR API KEY'\n",
    "endpoint = 'https://maps.googleapis.com/maps/api/directions/json?'\n",
    "\n",
    "def get_distance_duration(origin, destination, mode = 'driving'):\n",
    "    origin_str = f'{origin[0]},{origin[1]}'\n",
    "    destination_str = f'{destination[0]},{destination[1]}'\n",
    "    request_url = f'{endpoint}origin={origin_str}&destination={destination_str}&mode={mode}&key={maps_api_key}'\n",
    "    response = requests.get(request_url)\n",
    "    data = response.json()\n",
    "    \n",
    "    if data['status'] == 'OK':\n",
    "        road_distance_meters = data['routes'][0]['legs'][0]['distance']['value']\n",
    "        road_duration_seconds = data['routes'][0]['legs'][0]['duration']['value']\n",
    "\n",
    "        return road_distance_meters/1000, road_duration_seconds/3600\n",
    "        \n",
    "    else:\n",
    "        print('Error: Unable to calculate road distance and duration.')\n",
    "        return None\n",
    "\n",
    "def haversine(latlon1, latlon2):\n",
    "    lat1, lon1 = latlon1\n",
    "    lat2, lon2 = latlon2\n",
    "    R = 6371000  #radius of Earth in meters\n",
    "    phi_1 = radians(lat1)\n",
    "    phi_2 = radians(lat2)\n",
    "\n",
    "    delta_phi = radians(lat2 - lat1)\n",
    "    delta_lambda = radians(lon2 - lon1)\n",
    "\n",
    "    a = (np.sin(delta_phi / 2) ** 2 +\n",
    "         np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda / 2) ** 2)\n",
    "\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    meters = R * c  \n",
    "    return meters/1000 #output distance in km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86dc0aa-be9e-41ae-8f8e-cf9dcf321381",
   "metadata": {},
   "source": [
    "<h2>Particle Swarm optimizer class</h2>\n",
    "Functions wrapped in a class to apply the optimizer to custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eaec316-2d61-4341-8b02-d5227a1a2117",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParticleSwarmOptimizer():\n",
    "    PARAM_BOUNDS = (0, 10)\n",
    "\n",
    "    def __init__(self, lat_bounds, long_bounds, max_iter= 10, num_particles= 10, inertia= 0.5, cognitive_coef= 1.5, social_coef= 1.5):\n",
    "        self.LAT_BOUNDS = lat_bounds #(c1.geolocation_lat.min(), c1.geolocation_lat.max())\n",
    "        self.LON_BOUNDS = long_bounds #(c1.geolocation_lng.min(), c1.geolocation_lng.max())\n",
    "        self.MAX_ITERATIONS = max_iter\n",
    "        self.NUM_PARTICLES = num_particles\n",
    "        self.W = inertia  # Inertia weight\n",
    "        self.C1 = cognitive_coef  # Cognitive coefficient\n",
    "        self.C2 = social_coef  # Social coefficient\n",
    "        self.global_path = []\n",
    "        self.score_path = []\n",
    "    \n",
    "    def initialize_particles(self):\n",
    "        particle_positions = np.random.uniform([self.LAT_BOUNDS[0], self.LON_BOUNDS[0]] + [ParticleSwarmOptimizer.PARAM_BOUNDS[0]] * 3,\n",
    "                                               [self.LAT_BOUNDS[1], self.LON_BOUNDS[1]] + [ParticleSwarmOptimizer.PARAM_BOUNDS[1]] * 3,\n",
    "                                               (self.NUM_PARTICLES, 5))\n",
    "        particle_velocities = np.random.uniform(-1, 1, particle_positions.shape)\n",
    "        return particle_positions, particle_velocities\n",
    "\n",
    "    def update_velocity(self,velocity, personal_best, global_best, position):\n",
    "        r1, r2 = np.random.rand(), np.random.rand()\n",
    "        new_velocity = self.W * velocity + self.C1 * r1 * (personal_best - position) + self.C2 * r2 * (global_best - position)\n",
    "        return new_velocity\n",
    "    \n",
    "    def update_position(self, position, velocity):\n",
    "        new_position = position + velocity\n",
    "        new_position[0] = np.clip(new_position[0], self.LAT_BOUNDS[0], self.LAT_BOUNDS[1])  # Latitude\n",
    "        new_position[1] = np.clip(new_position[1], self.LON_BOUNDS[0], self.LON_BOUNDS[1])  # Longitude\n",
    "        # new_position[2:5] = np.clip(new_position[2:5], PARAM_BOUNDS[0], PARAM_BOUNDS[1])  # Alpha, Beta, Gamma\n",
    "        return new_position\n",
    "    \n",
    "    def update_features(self, particle, customer_locations):\n",
    "        dist = []\n",
    "        dur = []\n",
    "        hav = []\n",
    "        origin = (particle[0],particle[1])\n",
    "    \n",
    "        for loc in customer_locations:\n",
    "            hav.append(haversine(origin, tuple(loc)))\n",
    "            data = get_distance_duration(origin, tuple(loc))\n",
    "            \n",
    "            if data:\n",
    "                dist.append(data[0])\n",
    "                dur.append(data[1])\n",
    "    \n",
    "        return pd.Series(hav), pd.Series(dist), pd.Series(dur)\n",
    "    \n",
    "    def cost_function(self, position, haversine_distances, road_distances, road_durations, lambda_reg=0.01, epsilon=1e-6):\n",
    "        latitude, longitude, alpha, beta, gamma = position\n",
    "    \n",
    "        cost = (1 / len(haversine_distances)) * sum(\n",
    "            alpha * haversine_distances +\n",
    "            beta * road_distances +\n",
    "            gamma * road_durations\n",
    "        ) + lambda_reg * (alpha**2 + beta**2 + gamma**2)\n",
    "    \n",
    "        log_penalty = -np.sum(np.log(np.array([alpha, beta, gamma]) + epsilon))\n",
    "        cost = cost + log_penalty\n",
    "    \n",
    "        return cost\n",
    "\n",
    "    def fit_train(self, X, locs):\n",
    "        scaler = StandardScaler()\n",
    "        vals = scaler.fit_transform(np.c_[X.geo_distance, X.road_distance, X.road_duration])\n",
    "        initial_haversine_distances, initial_road_distances, initial_road_durations = vals[:,0], vals[:,1], vals[:,2]\n",
    "        \n",
    "        # Initialize particles and their velocities\n",
    "        self.particles, self.velocities = self.initialize_particles()\n",
    "        self.personal_best = self.particles.copy()\n",
    "        \n",
    "        \n",
    "        # Initialize the cost for each particle (using the initial distances and durations)\n",
    "        self.personal_best_scores = np.array([self.cost_function(\n",
    "            p, \n",
    "            initial_haversine_distances, \n",
    "            initial_road_distances, \n",
    "            initial_road_durations\n",
    "        ) for p in self.personal_best])\n",
    "        self.global_best = self.personal_best[np.argmin(self.personal_best_scores)]\n",
    "        self.global_best_score = min(self.personal_best_scores)\n",
    "        self.global_path.append(self.global_best)\n",
    "        self.score_path.append(self.global_best_score)\n",
    "        \n",
    "        for its in tqdm(range(self.MAX_ITERATIONS)):\n",
    "            print(f'Iteration no: {its}')\n",
    "            for i in tqdm(range(self.NUM_PARTICLES)):\n",
    "                self.velocities[i] = self.update_velocity(self.velocities[i], self.personal_best[i], self.global_best, self.particles[i])\n",
    "                self.particles[i] = self.update_position(self.particles[i], self.velocities[i])\n",
    "        \n",
    "                # Update the distances and durations for the current particle position\n",
    "                updated_haversine_distances, updated_road_distances, updated_road_durations = self.update_features(self.particles[i], locs)\n",
    "                vals = scaler.transform(np.c_[updated_haversine_distances, updated_road_distances, updated_road_durations])\n",
    "                updated_haversine_distances, updated_road_distances, updated_road_durations = vals[:,0], vals[:,1], vals[:,2]\n",
    "                \n",
    "                # Calculate the current cost with updated distances and durations\n",
    "                current_cost = self.cost_function(self.particles[i], updated_haversine_distances, updated_road_distances, updated_road_durations)\n",
    "        \n",
    "                if current_cost < self.personal_best_scores[i]:\n",
    "                    self.personal_best[i] = self.particles[i]\n",
    "                    self.personal_best_scores[i] = current_cost\n",
    "        \n",
    "                    if current_cost < self.global_best_score:\n",
    "                        self.global_best = self.particles[i]\n",
    "                        self.global_best_score = current_cost\n",
    "                        self.global_path.append(self.global_best)\n",
    "                        self.score_path.append(self.global_best_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1840621b-f8be-4b9e-a21d-1339ddd9fcbe",
   "metadata": {},
   "source": [
    "<h2>Cost and performance vs multiple iterations</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a9a549-1ba5-4209-9e14-8682f97e9d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = np.arange(2,25,2)\n",
    "best_loc = []\n",
    "best_score = []\n",
    "path = []\n",
    "for iter in tqdm(params):\n",
    "    pso = ParticleSwarmOptimizer(\n",
    "        lat_bounds= (c1.geolocation_lat.min(), c1.geolocation_lat.max()), \n",
    "        long_bounds= (c1.geolocation_lng.min(), c1.geolocation_lng.max()),\n",
    "        max_iter= iter,\n",
    "        num_particles= 5,\n",
    "    )\n",
    "    pso.fit_train(X= c1, locs= np.c_[np.array(c1.geolocation_lat), np.array(c1.geolocation_lng)])\n",
    "    best_loc.append(pso.global_best) \n",
    "    best_score.append(pso.global_best_score) \n",
    "    path.append((pso.global_path, pso.score_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94fdb559-db6f-47f3-bb20-3565e3dff417",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = [[positions[0],positions[1]] for positions in path[10][0]]\n",
    "coords.insert(0, [c1.centroid_lat.values[0],c1.centroid_lng.values[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7971103-3122-46d3-b90b-c3bb71d526fe",
   "metadata": {},
   "source": [
    "<h2>Plotting new warehouse location on map</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec20b853-c7d7-42b4-9764-342f2783ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapf = folium.Map(\n",
    "    location= [c1.geolocation_lat.mean(),c1.geolocation_lng.mean()],\n",
    "    zoom_start = 11.5,\n",
    "    tiles= 'OpenStreetMap',\n",
    "    height= 650\n",
    ")\n",
    "\n",
    "circles= c1.apply(\n",
    "    lambda row: folium.CircleMarker(\n",
    "        location= [row.geolocation_lat, row.geolocation_lng],\n",
    "        radius= 1,\n",
    "        popup= \"\"+str(row.geolocation_lat)+\", \"+str(row.geolocation_lng)+\"\\n\"+str(row.CLUSTER_hdbscan),\n",
    "        color= 'darkcyan',\n",
    "        fill= True,\n",
    "        fill_color= 'darkcyan',\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(mapf),\n",
    "    axis= 1\n",
    ")\n",
    "\n",
    "rect= folium.Rectangle(\n",
    "    bounds= [(c1.geolocation_lat.min(),c1.geolocation_lng.min()),(c1.geolocation_lat.max(), c1.geolocation_lng.max())]\n",
    ").add_to(mapf)\n",
    "\n",
    "cluster1= folium.CircleMarker(\n",
    "    location= [c1.centroid_lat.values[0],c1.centroid_lng.values[0]],\n",
    "    radius= 5,\n",
    "    color= 'tomato',\n",
    "    fill= True,\n",
    "    fill_color= 'tomato',\n",
    "    fill_opacity=0.5\n",
    ").add_to(mapf)\n",
    "\n",
    "cluster2= folium.CircleMarker(\n",
    "    location= [-16.70603564, -49.28418465],\n",
    "    radius= 5,\n",
    "    color= 'magenta',\n",
    "    fill= True,\n",
    "    fill_color= 'magenta',\n",
    "    fill_opacity=0.5\n",
    ").add_to(mapf)\n",
    "\n",
    "mapf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c814e36-75f8-4f89-ac43-6cf9eb6ad767",
   "metadata": {},
   "source": [
    "<h2>Plotting path to new warehouse location on map</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6e8d69-2436-4188-aee0-a2abbdc7937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapf = folium.Map(\n",
    "    location= [c1.geolocation_lat.mean(),c1.geolocation_lng.mean()],\n",
    "    zoom_start = 11.5,\n",
    "    tiles= 'OpenStreetMap',\n",
    "    height= 650\n",
    ")\n",
    "\n",
    "circles= c1.apply(\n",
    "    lambda row: folium.CircleMarker(\n",
    "        location= [row.geolocation_lat, row.geolocation_lng],\n",
    "        radius= 1,\n",
    "        popup= \"\"+str(row.geolocation_lat)+\", \"+str(row.geolocation_lng)+\"\\n\"+str(row.CLUSTER_hdbscan),\n",
    "        color= 'darkcyan',\n",
    "        fill= True,\n",
    "        fill_color= 'darkcyan',\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(mapf),\n",
    "    axis= 1\n",
    ")\n",
    "\n",
    "rect= folium.Rectangle(\n",
    "    bounds= [(c1.geolocation_lat.min(),c1.geolocation_lng.min()),(c1.geolocation_lat.max(), c1.geolocation_lng.max())]\n",
    ").add_to(mapf)\n",
    "\n",
    "cluster1= folium.CircleMarker(\n",
    "    location= [c1.centroid_lat.values[0],c1.centroid_lng.values[0]],\n",
    "    radius= 5,\n",
    "    color= 'tomato',\n",
    "    fill= True,\n",
    "    fill_color= 'tomato',\n",
    "    fill_opacity=0.5\n",
    ").add_to(mapf)\n",
    "\n",
    "cluster2= folium.CircleMarker(\n",
    "    location= [-16.706036, -49.284185],\n",
    "    radius= 5,\n",
    "    color= 'magenta',\n",
    "    fill= True,\n",
    "    fill_color= 'magenta',\n",
    "    fill_opacity=0.5\n",
    ").add_to(mapf)\n",
    "\n",
    "poltLines = folium.PolyLine(\n",
    "    locations= coords,\n",
    "    color= 'darkolivegreen'\n",
    ").add_to(mapf)\n",
    "\n",
    "poltPoints = [folium.CircleMarker(\n",
    "    location= coord,\n",
    "    radius= 3,\n",
    "    color= 'tomato',\n",
    "    fill= True,\n",
    "    fill_color= 'tomato',\n",
    "    fill_opacity=0.5\n",
    ").add_to(mapf) for coord in coords]\n",
    "\n",
    "mapf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
