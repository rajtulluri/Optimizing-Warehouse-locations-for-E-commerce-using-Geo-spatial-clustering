{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "import folium\n",
    "import random\n",
    "import seaborn as sns\n",
    "from math import radians\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.cluster import KMeans, DBSCAN, HDBSCAN\n",
    "from sklearn.metrics import silhouette_score,  davies_bouldin_score, calinski_harabasz_score, make_scorer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from plotly.colors import DEFAULT_PLOTLY_COLORS, PLOTLY_SCALES\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.float_format', lambda x: '%.6f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Clustering techniques and grid search</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pallete = \"\"\"aliceblue, aqua, aquamarine, azure,\n",
    "beige, bisque, blanchedalmond, blue,\n",
    "blueviolet, brown, burlywood, cadetblue,\n",
    "chartreuse, chocolate, coral, cornflowerblue,\n",
    "cornsilk, crimson, cyan, darkblue, darkcyan,\n",
    "darkgoldenrod, darkgray, darkgrey, darkgreen,\n",
    "darkkhaki, darkmagenta, darkolivegreen, darkorange,\n",
    "darkorchid, darkred, darksalmon, darkseagreen,\n",
    "darkslateblue, darkslategray, darkslategrey,\n",
    "darkturquoise, darkviolet, deeppink, deepskyblue,\n",
    "dimgray, dimgrey, dodgerblue, firebrick,\n",
    "floralwhite, forestgreen, fuchsia, gainsboro,\n",
    "ghostwhite, gold, goldenrod, gray, grey, green,\n",
    "greenyellow, honeydew, hotpink, indianred, indigo,\n",
    "ivory, khaki, lavender, lavenderblush, lawngreen,\n",
    "lemonchiffon, lightblue, lightcoral, lightcyan,\n",
    "lightgoldenrodyellow, lightgray, lightgrey,\n",
    "lightgreen, lightpink, lightsalmon, lightseagreen,\n",
    "lightskyblue, lightslategray, lightslategrey,\n",
    "lightsteelblue, lightyellow, lime, limegreen,\n",
    "linen, magenta, maroon, mediumaquamarine,\n",
    "mediumblue, mediumorchid, mediumpurple,\n",
    "mediumseagreen, mediumslateblue, mediumspringgreen,\n",
    "mediumturquoise, mediumvioletred, midnightblue,\n",
    "mintcream, mistyrose, moccasin, navajowhite, navy,\n",
    "oldlace, olive, olivedrab, orange, orangered,\n",
    "orchid, palegoldenrod, palegreen, paleturquoise,\n",
    "palevioletred, papayawhip, peachpuff, peru, pink,\n",
    "plum, powderblue, purple, red, rosybrown,\n",
    "royalblue, saddlebrown, salmon, sandybrown,\n",
    "seagreen, seashell, sienna, silver, skyblue,\n",
    "slateblue, slategray, slategrey, snow, springgreen,\n",
    "steelblue, tan, teal, thistle, tomato, turquoise,\n",
    "violet, wheat, yellow,\n",
    "yellowgreen\"\"\".split(',')\n",
    "\n",
    "colors = [s.replace('\\n','').strip() for s in pallete]\n",
    "l = len(colors)\n",
    "random.shuffle(colors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def haversine(latlon1, latlon2):\n",
    "    lat1, lon1 = latlon1\n",
    "    lat2, lon2 = latlon2\n",
    "    R = 6371000  # radius of Earth in meters\n",
    "    phi_1 = radians(lat1)\n",
    "    phi_2 = radians(lat2)\n",
    "\n",
    "    delta_phi = radians(lat2 - lat1)\n",
    "    delta_lambda = radians(lon2 - lon1)\n",
    "\n",
    "    a = (np.sin(delta_phi / 2) ** 2 +\n",
    "         np.cos(phi_1) * np.cos(phi_2) * np.sin(delta_lambda / 2) ** 2)\n",
    "\n",
    "    c = 2 * np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "\n",
    "    meters = R * c  # output distance in meters\n",
    "    return meters/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('.\\Olist_data/customer_data.csv', index_col= 'Unnamed: 0')\n",
    "data.dropna(inplace= True)\n",
    "df = data.drop_duplicates(subset= ['geolocation_lat','geolocation_lng'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapf = folium.Map(\n",
    "    location= [df.geolocation_lat.mean(), df.geolocation_lng.mean()],\n",
    "    zoom_start= 4,\n",
    "    tiles= 'OpenStreetMap',\n",
    "    height= 550,\n",
    ")\n",
    "\n",
    "circles = df.apply(\n",
    "    lambda row: folium.CircleMarker(\n",
    "        location= [row.geolocation_lat, row.geolocation_lng],\n",
    "        radius= 1,\n",
    "        fill= True,\n",
    "        fill_color= '#009999',\n",
    "        color= '#009999',\n",
    "        popup= \"\"+str(row.geolocation_lat)+\", \"+str(row.geolocation_lng)\n",
    "    ).add_to(mapf),\n",
    "    axis= 1\n",
    ")\n",
    "# mapf.save('/home/raj/All_locations.html')\n",
    "mapf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>DBSCAN clustering algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DBSCAN(eps= 8, min_samples= 15, metric= haversine).fit(df[['geolocation_lat','geolocation_lng']])\n",
    "df['CLUSTER_dbscan']= model.labels_\n",
    "\n",
    "mapf = folium.Map(\n",
    "    location= [df.geolocation_lat.mean(),df.geolocation_lng.mean()],\n",
    "    zoom_start = 3.5,\n",
    "    tiles= 'OpenStreetMap',\n",
    "    height= 500\n",
    ")\n",
    "\n",
    "circles= df.apply(\n",
    "    lambda row: folium.CircleMarker(\n",
    "        location= [row.geolocation_lat, row.geolocation_lng],\n",
    "        radius= 1,\n",
    "        popup= \"\"+str(row.geolocation_lat)+\", \"+str(row.geolocation_lng)+\"\\n\"+str(row.CLUSTER_dbscan),\n",
    "        color= [colors[row.CLUSTER_dbscan % l] if row.CLUSTER_dbscan != -1 else '#00000F'],\n",
    "        fill= True,\n",
    "        fill_color= [colors[row.CLUSTER_dbscan % l] if row.CLUSTER_dbscan != -1 else '#00000F']\n",
    "    ).add_to(mapf),\n",
    "    axis= 1\n",
    ")\n",
    "# mapf.save('/home/raj/DBSCAN_clustering.html')\n",
    "mapf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= [\"geolocation_lat\",\"geolocation_lng\"]\n",
    "\n",
    "print(\"DBSCAN metrics\")\n",
    "print(f'Number of clusters: {len(np.unique(df.CLUSTER_dbscan))}')\n",
    "print(f'Number of outliers: {len(df[df.CLUSTER_dbscan == -1])}')\n",
    "\n",
    "# pred= [(c+2)*x if x == -1 else x for c,x in enumerate(df.CLUSTER_dbscan)]\n",
    "# print(f'Silhouette score, with outliers as singletons: {silhouette_score(df[cols],pred,metric= haversine)}')\n",
    "\n",
    "temp = df[df.CLUSTER_dbscan != -1]\n",
    "print(f'Silhouette score without outliers: {silhouette_score(temp[cols],temp.CLUSTER_dbscan, metric=haversine)}')\n",
    "\n",
    "davies_bouldin = davies_bouldin_score(temp[cols], temp['CLUSTER_dbscan'])\n",
    "print(f'Davies-Bouldin Index without outliers: {davies_bouldin}')\n",
    "\n",
    "# Calculate Calinski-Harabasz Index\n",
    "calinski_harabasz = calinski_harabasz_score(temp[cols], temp['CLUSTER_dbscan'])\n",
    "print(f'Calinski-Harabasz Index without outliers: {calinski_harabasz}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps_values = list(range(1,30,5))\n",
    "min_samples_values = list(range(5,50,5))\n",
    "\n",
    "all_scores = {}\n",
    "\n",
    "run_count = 0\n",
    "\n",
    "for eps in tqdm(eps_values):\n",
    "    for min_samples in tqdm(min_samples_values):\n",
    "        print(run_count, eps, min_samples)\n",
    "        grid_df = df[['geolocation_lat','geolocation_lng']].copy()\n",
    "        \n",
    "        dbscan = DBSCAN(eps=eps, min_samples=min_samples, metric= haversine)\n",
    "        labels = dbscan.fit_predict(grid_df)\n",
    "        \n",
    "        grid_df['cluster'] = labels\n",
    "        grid_df = grid_df[grid_df.cluster != -1]\n",
    "        \n",
    "        print(grid_df.shape, len(grid_df.cluster.unique()))\n",
    "        \n",
    "        if  len(grid_df.cluster.unique()) > 1:\n",
    "            silhouette = silhouette_score(grid_df[['geolocation_lat','geolocation_lng']], grid_df.cluster, metric= haversine)\n",
    "            davies_bouldin = davies_bouldin_score(grid_df[['geolocation_lat','geolocation_lng']], grid_df.cluster)\n",
    "            calinski_harabasz = calinski_harabasz_score(grid_df[['geolocation_lat','geolocation_lng']], grid_df.cluster)\n",
    "\n",
    "            print(silhouette, davies_bouldin, calinski_harabasz)\n",
    "\n",
    "            all_scores[f'run_{run_count+1}'] = {\n",
    "                'silhouette_score': silhouette,\n",
    "                'davies_bouldin_score': davies_bouldin,\n",
    "                'calinski_harabasz_score': calinski_harabasz,\n",
    "                'eps': eps,\n",
    "                'min_sample': min_samples\n",
    "            }\n",
    "            \n",
    "        run_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = pd.read_csv('.\\Olist_data/dbscan_grid_search_results.csv')\n",
    "db.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the scores stored in arrays like silhouette_scores, davies_bouldin_scores, calinski_harabasz_scores\n",
    "\n",
    "num_runs = len(db)  # Assuming the number of runs is the same for all scores\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 12))\n",
    "\n",
    "# Silhouette Score\n",
    "axs[0].plot(np.arange(1, num_runs + 1), db.silhouette_score, marker='o')\n",
    "axs[0].set_title('Silhouette Score')\n",
    "\n",
    "# Davies Bouldin Score\n",
    "axs[1].plot(np.arange(1, num_runs + 1), db.davies_bouldin_score, marker='o')\n",
    "axs[1].set_title('Davies Bouldin Score')\n",
    "\n",
    "# Calinski Harabasz Score\n",
    "axs[2].plot(np.arange(1, num_runs + 1), db.calinski_harabasz_score, marker='o')\n",
    "axs[2].set_title('Calinski Harabasz Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>HDBSCAN Clustering algorithm</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HDBSCAN(min_cluster_size= 5, min_samples= 5, alpha= 0.1, cluster_selection_epsilon= 3, metric= haversine).fit(df[['geolocation_lat','geolocation_lng']])\n",
    "df['CLUSTER_hdbscan']= model.labels_\n",
    "\n",
    "mapf = folium.Map(\n",
    "    location= [df.geolocation_lat.mean(),df.geolocation_lng.mean()],\n",
    "    zoom_start = 3.5,\n",
    "    tiles= 'OpenStreetMap',\n",
    "    height= 500\n",
    ")\n",
    "\n",
    "folium.GeoJson(filtered_geojson, name='world_borders', style_function= style_function).add_to(mapf)\n",
    "folium.GeoJson(brazil_geojson, name='brazil_borders', style_function= style_function_brazil).add_to(mapf)\n",
    "\n",
    "circles= df.apply(\n",
    "    lambda row: folium.CircleMarker(\n",
    "        location= [row.geolocation_lat, row.geolocation_lng],\n",
    "        radius= 1,\n",
    "        popup= \"\"+str(row.geolocation_lat)+\", \"+str(row.geolocation_lng)+\"\\n\"+str(row.CLUSTER_hdbscan),\n",
    "        color= [colors[row.CLUSTER_hdbscan % l] if row.CLUSTER_hdbscan != -1 else '#00000F'],\n",
    "        fill= True,\n",
    "        fill_color= [colors[row.CLUSTER_hdbscan % l] if row.CLUSTER_hdbscan != -1 else '#00000F'],\n",
    "        fill_opacity=0.5\n",
    "    ).add_to(mapf),\n",
    "    axis= 1\n",
    ")\n",
    "# mapf.save('/home/raj/DBSCAN_clustering.html')\n",
    "mapf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols= [\"geolocation_lat\",\"geolocation_lng\"]\n",
    "\n",
    "temp = df[df.CLUSTER_hdbscan != -1]\n",
    "distances = haversine_distances(temp[cols]) * 6371000/1000\n",
    "\n",
    "print(f'Silhouette score without outliers: {silhouette_score(temp[cols], temp[\"CLUSTER_hdbscan\"], metric=haversine)}')\n",
    "\n",
    "davies_bouldin = davies_bouldin_score(temp[cols], temp['CLUSTER_hdbscan'])\n",
    "print(f'Davies-Bouldin Index without outliers: {davies_bouldin}')\n",
    "\n",
    "# Calculate Calinski-Harabasz Index\n",
    "calinski_harabasz = calinski_harabasz_score(temp[cols], temp['CLUSTER_hdbscan'])\n",
    "print(f'Calinski-Harabasz Index without outliers: {calinski_harabasz}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size= [5, 10, 15, 20, 25, 30, 35, 40]\n",
    "min_samples= [1, 2, 5, 10]\n",
    "alpha= [0.1, 0.5, 1.0]\n",
    "cluster_selection_epsilon= [3, 5, 10, 20]\n",
    "\n",
    "perms = list(product(min_samples, alpha, cluster_selection_epsilon, min_cluster_size))\n",
    "\n",
    "all_scores = {}\n",
    "run_count = 0\n",
    "\n",
    "for min_sample, alp, eps, min_cluster in tqdm(perms):\n",
    "        print(run_count, min_cluster, min_sample, alp, eps)\n",
    "        grid_df = df[['geolocation_lat','geolocation_lng']].copy()\n",
    "        \n",
    "        hdbscan = HDBSCAN(min_cluster_size= min_cluster, min_samples= min_sample, alpha= alp, cluster_selection_epsilon= eps,\n",
    "                        metric= 'precomputed', n_jobs= 2)\n",
    "        labels = hdbscan.fit(distances).labels_\n",
    "        \n",
    "        grid_df['cluster'] = pd.Series(labels, index= grid_df.index)\n",
    "        grid_df = grid_df[grid_df.cluster != -1]\n",
    "        \n",
    "        print(grid_df.shape, len(grid_df.cluster.unique()))\n",
    "        \n",
    "        if  len(grid_df.cluster.unique()) > 1:\n",
    "            d = haversine_distances(grid_df[['geolocation_lat','geolocation_lng']])  * 6371000/1000\n",
    "            silhouette = silhouette_score(d, grid_df.cluster, metric= 'precomputed')\n",
    "            davies_bouldin = davies_bouldin_score(grid_df[['geolocation_lat','geolocation_lng']], grid_df.cluster)\n",
    "            calinski_harabasz = calinski_harabasz_score(grid_df[['geolocation_lat','geolocation_lng']], grid_df.cluster)\n",
    "\n",
    "            print(silhouette, davies_bouldin, calinski_harabasz)\n",
    "\n",
    "            all_scores[f'run_{run_count+1}'] = {\n",
    "                'silhouette_score': silhouette,\n",
    "                'davies_bouldin_score': davies_bouldin,\n",
    "                'calinski_harabasz_score': calinski_harabasz,\n",
    "                'eps': eps,\n",
    "                'min_sample': min_sample,\n",
    "                'alpha': alp,\n",
    "                'min_cluster_size': min_cluster\n",
    "            }\n",
    "            \n",
    "        run_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdb = pd.read_csv('./Olist_data/hdbscan_grid_search_results.csv')\n",
    "hdb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you have the scores stored in arrays like silhouette_scores, davies_bouldin_scores, calinski_harabasz_scores\n",
    "\n",
    "num_runs = len(hdb)  # Assuming the number of runs is the same for all scores\n",
    "\n",
    "fig, axs = plt.subplots(3, 1, figsize=(10, 12))\n",
    "\n",
    "# Silhouette Score\n",
    "axs[0].plot(np.arange(1, num_runs + 1), hdb.silhouette_score, marker='o')\n",
    "axs[0].set_title('Silhouette Score')\n",
    "\n",
    "# Davies Bouldin Score\n",
    "axs[1].plot(np.arange(1, num_runs + 1), hdb.davies_bouldin_score, marker='o')\n",
    "axs[1].set_title('Davies Bouldin Score')\n",
    "\n",
    "# Calinski Harabasz Score\n",
    "axs[2].plot(np.arange(1, num_runs + 1), hdb.calinski_harabasz_score, marker='o')\n",
    "axs[2].set_title('Calinski Harabasz Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
